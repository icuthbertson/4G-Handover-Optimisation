\chapter{Future Work}\label{future work}
So while the optimisation system created was able to make vast improvements to the performance of the simulated network, there was still the potential for the system to perform even better. Such ways that could possibly be explored to improve the system could be to add more complexity to the simulation and simulated network, as well as improving the machine learning algorithm used as a whole.

\section{Slow Fading}
An important expansion to this project would be the addition of Slow Fading to the Path Loss equations. This addition would simulate obstacles in the simulation environment such as wall and vehicles. This would add more randomness into the simulation and add the possibility of sudden drops in the signal strength resulting in dropped calls or sudden improvements triggering a handover only for the signal strength to drop again resulting in a handover ping-pong. This would be a good addition to the project because it would make the simulation more realistic.

\section{Limited Resources Within the Network}
A possible expansion on this project would be to have limited resources within the simulated network. This would mean that instead of the base stations having the resources to accommodate all the UEs within the simulation they would only have the resources for a limited number of UEs. This would create a new dynamic within the simulation where handovers could be refused and this in turn could increase the number of dropped calls. This could, therefore, mean that the Q-Learning agents for the base stations reduce the values of TTT and hys so that it takes less time to trigger a handover. This means that UEs connected to a base station will handover to another base station faster freeing up the resources for another UE. 

\section{Improved Algorithm}
Another important expansion of the project would be to improve the machine learning algorithm. It was seen that while the Q-Learning algorithm used did produce improved performance compared to if no machine learning was used; there was still a lot of improvement that could have taken place as the algorithm kept oscillating between what could be assumed to be non-optimal values as the oscillations were very frequent. Such improvements that could be made to the algorithm would be involve finding a way to `break' out of the states that produced the oscillations. Other improvements would be providing mechanisms to ensure that all possible actions from every state were explored during the process of learning the environment. The algorithm currently learns the environment randomly which means that, while a lot of time was given to learning the environment, there is no insurance that all, or a large majority, of the states and actions were actually explored.
